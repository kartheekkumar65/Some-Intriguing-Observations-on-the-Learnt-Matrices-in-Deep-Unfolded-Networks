{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "import time\n",
    "import datetime\n",
    "from scipy import stats\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(D, noise_Std_Dev, k, p, rng):\n",
    "    print(\" noise level is \", noise_Std_Dev)\n",
    "    m, n = D.shape\n",
    "    x = rng.normal(0, 1, (n, p)) * rng.binomial(1, k, (n, p))\n",
    "    # x = rng.binomial(1,k,(n,p))\n",
    "    # mask = rng.uniform(0,1,(n,p))\n",
    "    # x[mask > 0.5 ] *= -1\n",
    "\n",
    "    y = D @ x\n",
    "    # y = torch.tensor(y)\n",
    "    noise = rng.normal(0, noise_Std_Dev, y.shape)\n",
    "    y = y + noise\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def pes(x, x_est):\n",
    "    d = []\n",
    "    for i in range(x.shape[1]):\n",
    "        M = max(np.sum(x[:, i] != 0), np.sum(x_est[:, i] != 0))\n",
    "        pes_ = (M - np.sum((x[:, i] != 0) * (x_est[:, i] != 0))) / M\n",
    "        if not np.isnan(pes_):\n",
    "            d.append(pes_)\n",
    "        else:\n",
    "            print(M)\n",
    "            print(\"nan is found here\")\n",
    "    return np.mean(d), np.std(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_thr(x, thr):\n",
    "    \"\"\"Applies soft thresholding to the input tensor x with threshold thr.\"\"\"\n",
    "    return torch.sign(x) * torch.relu(torch.abs(x) - thr)\n",
    "\n",
    "\n",
    "class LISTA(nn.Module):\n",
    "    def __init__(self, m, n, Dict, numIter, device='cpu', thr=None, no_activation=False):\n",
    "        super(LISTA, self).__init__()\n",
    "        self.numIter = numIter\n",
    "        self.no_activation = no_activation\n",
    "        # compute the max eigen value of the D'*D\n",
    "        self.alpha = (np.linalg.norm(Dict, 2) ** 2) * 1.001\n",
    "        self.thr = thr\n",
    "        self.device = device\n",
    "\n",
    "        # Setting up the linear layers with specific dimensions and without bias\n",
    "        self._W = nn.Linear(in_features=m, out_features=n, bias=False)\n",
    "        self._S = nn.Linear(in_features=n, out_features=n, bias=False)\n",
    "\n",
    "        # Shrinkage thresholds, one per iteration\n",
    "        self.thr = nn.Parameter(torch.rand(numIter, 1), requires_grad=True)\n",
    "        self.A = Dict\n",
    "\n",
    "    def weights_init(self):\n",
    "        \"\"\"Initializes weights for the _W and _S matrices based on the provided dictionary A and scaling factor alpha.\"\"\"\n",
    "        A = self.A\n",
    "        alpha = self.alpha\n",
    "\n",
    "        # Compute the initial weights for _W and _S based on the dictionary and alpha\n",
    "        S = torch.from_numpy(np.eye(A.shape[1]) - (1 / alpha) * np.matmul(A.T, A))\n",
    "        S = S.float().to(self.device)\n",
    "        B = torch.from_numpy((1 / alpha) * A.T)\n",
    "        B = B.float().to(self.device)\n",
    "\n",
    "        thr = torch.ones(self.numIter, 1) * 0.1 / alpha\n",
    "\n",
    "        # Setting the weights of _S and _W layers\n",
    "        self._S.weight = nn.Parameter(S)\n",
    "        self._W.weight = nn.Parameter(B)\n",
    "        self.thr.data = thr\n",
    "\n",
    "    def forward(self, y):\n",
    "        \"\"\"Forward pass of LISTA, performing iterative thresholding.\"\"\"\n",
    "        x = []\n",
    "        d = torch.zeros(y.shape[0], self.A.shape[1], device=self.device)\n",
    "\n",
    "        for iter in range(self.numIter):\n",
    "            if self.no_activation:\n",
    "                d = self._W(y) + self._S(d)\n",
    "            else:\n",
    "                d = soft_thr(self._W(y) + self._S(d), self.thr[iter])\n",
    "\n",
    "            x.append(d)\n",
    "        return x\n",
    "\n",
    "def LISTA_test(net, Y, D, device):\n",
    "    \n",
    "    # convert the data into tensors\n",
    "    Y_t = torch.from_numpy(Y.T)\n",
    "    if len(Y.shape) <= 1:\n",
    "        Y_t = Y_t.view(1, -1)\n",
    "    Y_t = Y_t.float().to(device)\n",
    "    D_t = torch.from_numpy(D.T)\n",
    "    D_t = D_t.float().to(device)\n",
    "\n",
    "    ratio = 1\n",
    "    with torch.no_grad():\n",
    "        # Compute the output\n",
    "        net.eval()\n",
    "        X_lista = net(Y_t.float())\n",
    "        if len(Y.shape) <= 1:\n",
    "            X_lista = X_lista.view(-1)\n",
    "        X_final = X_lista[-1].cpu().numpy()\n",
    "        X_final = X_final.T\n",
    "\n",
    "    return X_final, X_lista\n",
    "\n",
    "seed = 80\n",
    "print(\"Seed: \", seed)\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "m = 70\n",
    "n = 100\n",
    "# create the random matrix\n",
    "D = rng.normal(0, 1 / np.sqrt(m), [m, n])\n",
    "D /= np.linalg.norm(D, 2, axis=0)\n",
    "\n",
    "\n",
    "input_SNR = 0.0 \n",
    "sparsity = 10\n",
    "numTest = 100\n",
    "X_test, Y_test = data_gen(D, input_SNR, sparsity / 100, numTest, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def science_plot_squared_norm_of_recon(squared_norms, step=1, filename=None):\n",
    "    # Plotting the stem plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    layers_index = np.arange(1, len(squared_norms) + 1, step)\n",
    "    plt.stem(\n",
    "        layers_index,\n",
    "        squared_norms[::step],\n",
    "        linefmt=\"r-\",\n",
    "        markerfmt=\"bo\",\n",
    "        basefmt=\"gray\",\n",
    "    )\n",
    "\n",
    "    # Labeling the axes\n",
    "    plt.xlabel(\"Layer Index\")\n",
    "    plt.ylabel(r\"$\\|\\hat{x}\\|_2^2$\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", antialiased=True)\n",
    "\n",
    "    if filename:\n",
    "        plt.savefig(filename, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "    # Adding a title\n",
    "    # plt.title('Squared Norm of Reconstructed X for Each Layer')\n",
    "\n",
    "    # Showing the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_squared_norm_of_recon(squared_norms, step=1, filename=None):\n",
    "    # Plotting the stem plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    layers_index = np.arange(0, len(squared_norms), step)\n",
    "    # squared_norms = np.ndarray(squared_norms)\n",
    "    # plt.stem(layers_index, squared_norms[::step], linefmt='r-', markerfmt='bo', basefmt=\"gray\")\n",
    "    plot_signal(\n",
    "        layers_index,\n",
    "        squared_norms[::step],\n",
    "        xlimits=[-1, len(squared_norms)],\n",
    "        ylimits=[np.min(squared_norms) - 1, min(1e18, np.max(squared_norms) + 1)],\n",
    "        xaxis_label=\"Layer Index\",\n",
    "        yaxis_label=r\"$\\|X_{recon}\\|^2$\",\n",
    "        grid=True,\n",
    "        save=filename,\n",
    "        axis_formatter=None\n",
    "    )\n",
    "\n",
    "def plot_signal(\n",
    "    x,\n",
    "    y,\n",
    "    ax=None,\n",
    "    plot_colour=\"blue\",\n",
    "    alpha=1,\n",
    "    xaxis_label=None,\n",
    "    yaxis_label=None,\n",
    "    title_text=None,\n",
    "    legend_label=None,\n",
    "    legend_show=True,\n",
    "    legend_loc=\"lower left\",\n",
    "    n_col=2,\n",
    "    line_style=\"-\",\n",
    "    line_width=None,\n",
    "    xlimits=[-2, 2],\n",
    "    ylimits=[-2, 2],\n",
    "    axis_formatter=\"%0.1f\",\n",
    "    show=False,\n",
    "    save=None,\n",
    "    annotates=False,\n",
    "    annotation=None,\n",
    "    pos=None,\n",
    "    marker=None,\n",
    "    markersize=10,\n",
    "    grid=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots signal with abscissa in x and ordinates in y\n",
    "\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "        ax = plt.gca()\n",
    "\n",
    "    plt.plot(\n",
    "        x,\n",
    "        y,\n",
    "        linestyle=line_style,\n",
    "        linewidth=line_width,\n",
    "        color=plot_colour,\n",
    "        label=legend_label,\n",
    "        zorder=0,\n",
    "        alpha=alpha,\n",
    "        marker=marker,\n",
    "        markersize=markersize,\n",
    "    )\n",
    "    if legend_label and legend_show:\n",
    "        plt.legend(\n",
    "            ncol=n_col, loc=legend_loc, frameon=False, framealpha=0.8, facecolor=\"white\"\n",
    "        )\n",
    "\n",
    "    if grid:\n",
    "        # plt.grid()\n",
    "        plt.grid(True, ls=\"--\", lw=0.5, c=\"k\", alpha=0.2)\n",
    "\n",
    "    plt.xlim(xlimits)\n",
    "    plt.ylim(ylimits)\n",
    "    plt.xlabel(xaxis_label)\n",
    "    plt.ylabel(yaxis_label)\n",
    "    plt.title(title_text)\n",
    "\n",
    "    if annotates:\n",
    "        plt.annotate(annotation, xy=pos, color=plot_colour)\n",
    "\n",
    "    if axis_formatter:\n",
    "        ax.yaxis.set_major_formatter(ticker.FormatStrFormatter(axis_formatter))\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(save + \".pdf\", format=\"pdf\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "def evaluate_model(model, save=False):\n",
    "    # Sparse Estimation using LISTA (testing phase)\n",
    "    PES_list = []\n",
    "    X_l2_squared_norm_list = []\n",
    "    X_l2_squared_norm_gt_list = []\n",
    "    SNR_list = []\n",
    "\n",
    "    X_out, X_LISTA = LISTA_test(model, Y_test, D, \"cpu\")\n",
    "\n",
    "    N_test = Y_test.shape[1]\n",
    "    for i in range(N_test):\n",
    "\n",
    "        err = np.linalg.norm(X_out[:, i] - X_test[:, i])\n",
    "        RSNR = 20 * np.log10(np.linalg.norm(X_test[:, i]) / err)\n",
    "        if np.isnan(RSNR):\n",
    "            print(\"!!! nan found\")\n",
    "            break\n",
    "        SNR_list.append(RSNR)\n",
    "\n",
    "        # Calculate the L2 squared norm of X_out\n",
    "        # l2_squared_norm = np.linalg.norm(X_out[:, i], ord=2) ** 2\n",
    "        l2_squared_norm = X_out[:, i].T @ X_out[:, i]\n",
    "        X_l2_squared_norm_list.append(l2_squared_norm)\n",
    "\n",
    "        l2_squared_norm = X_test[:, i].T @ X_test[:, i]\n",
    "        X_l2_squared_norm_gt_list.append(l2_squared_norm)\n",
    "\n",
    "    PES_mean, PES_std = pes(X_test, X_out)\n",
    "\n",
    "    SNR_list_LISTA = np.array(SNR_list)\n",
    "\n",
    "    L2_squared_list_LISTA = np.array(X_l2_squared_norm_list)\n",
    "\n",
    "    # Calculate and format LISTA-related statistics\n",
    "    avg_snr = round(np.mean(SNR_list_LISTA), 4)\n",
    "    std_snr = round(np.std(SNR_list_LISTA), 4)\n",
    "    max_snr = np.max(SNR_list_LISTA)\n",
    "\n",
    "    avg_pes = round(PES_mean, 4)\n",
    "    std_pes = round(PES_std, 4)\n",
    "\n",
    "    avg_l2_squared = round(np.mean(L2_squared_list_LISTA), 4)\n",
    "    std_l2_squared = round(np.std(L2_squared_list_LISTA), 4)\n",
    "    max_l2_squared = round(np.max(L2_squared_list_LISTA), 4)\n",
    "\n",
    "    avg_l2_squared = round(np.linalg.norm(X_out, \"fro\") ** 2 / X_out.shape[1], 4)\n",
    "    avg_l2_squared_GT = round(np.linalg.norm(X_test, \"fro\") ** 2 / X_out.shape[1], 4)\n",
    "    std_l2_squared = round(np.std(L2_squared_list_LISTA), 4)\n",
    "    max_l2_squared = round(np.max(L2_squared_list_LISTA), 4)\n",
    "\n",
    "    x_layer_i_list = []\n",
    "    for x_layer_i in X_LISTA:\n",
    "        l2_norms = torch.norm(\n",
    "            x_layer_i, p=2, dim=1\n",
    "        )  # Calculate the L2 norm for each sample in the layer\n",
    "        average_l2_norm = torch.mean(l2_norms)  # Compute the average L2 norm\n",
    "        x_layer_i_list.append(\n",
    "            average_l2_norm.item()\n",
    "        )  # Append the average to the list, converting tensor to scalar\n",
    "\n",
    "    print(f\"Testing: my LISTA average SNR: {avg_snr}\")\n",
    "    print(f\"Testing: my LISTA standard deviation in SNR: {std_snr}\")\n",
    "    print(f\"Testing: my LISTA peak SNR: {max_snr}\")\n",
    "\n",
    "    print(f\"Testing: my LISTA average X_out L2^2: {avg_l2_squared :.3f}\")\n",
    "    print(f\"Testing: my LISTA average X_test L2^2: {avg_l2_squared_GT :.3f}\")\n",
    "    print(f\"Testing: my LISTA standard deviation in X_out L2^2: {std_l2_squared}\")\n",
    "    print(f\"Testing: my LISTA peak X_out L2^2: {max_l2_squared}\")\n",
    "    print(f\"Testing: my LISTA mode X_out L2^2: {stats.mode(X_l2_squared_norm_list)}\")\n",
    "    print(\n",
    "        f\"Testing: my LISTA peak X_test L2^2: {stats.mode(X_l2_squared_norm_gt_list)}\"\n",
    "    )\n",
    "\n",
    "    print(f\"Testing: my LISTA average PES: {avg_pes}\")\n",
    "    print(f\"Testing: my LISTA standard deviation in PES: {std_pes}\")\n",
    "\n",
    "    print(\"-\" * 40)  # Divider line for better visual separation\n",
    "\n",
    "    if save:\n",
    "        with open(\n",
    "            f\"logs/01_test_stability_L{model.numIter}_act_{not model.no_activation}.txt\",\n",
    "            \"w\",\n",
    "        ) as f:\n",
    "            f.write(f\"Learned threshold: {model.thr.T}\\n\")\n",
    "            f.write(f\"Input SNR: {input_SNR}\\n\")\n",
    "            f.write(f\"Sparsity: {sparsity}\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(\n",
    "                f\"Average SNR: {avg_snr}, Standard deviation SNR: {std_snr}, Max SNR: {max_snr}\\n\"\n",
    "            )\n",
    "            f.write(\n",
    "                f\"Average X_out^2: {avg_l2_squared}, Standard deviation X_out^2: {std_l2_squared}, Max X_out^2: {max_l2_squared}\\n\"\n",
    "            )\n",
    "            f.write(f\"Average PES: {avg_pes}, Standard deviation PES: {std_pes}\\n\")\n",
    "\n",
    "    return X_out, X_test, x_layer_i_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numIter_list = [15, 100, 500, 1000]  # Number of iterations\n",
    "steps = [1, 2, 10, 15]\n",
    "# numIter = 15  # Number of iterations\n",
    "# thr_list = [0.001, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05, 0.06, 0.07]\n",
    "# thr_list = np.arange(0.04, 0.06, 0.001)\n",
    "\n",
    "thr_list = [0.045, 0.045, 0.055, 0.055]\n",
    "\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(\"../weights/model_parameters.pth\")\n",
    "\n",
    "for numIter, thr, step in zip(numIter_list, thr_list, steps):\n",
    "    print(\"*\" * 40)\n",
    "    print(f\"Num Iter: {numIter}\")\n",
    "    print(f\"Thr: {thr}\")\n",
    "    state_dict[\"thr\"] = torch.ones(numIter, 1) * thr\n",
    "\n",
    "    model = LISTA(m, n, D, numIter)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    X_out, X_test, x_layer_i_list = evaluate_model(model, save=False)\n",
    "    filename = f\"../figures/03_Impact_of_Soft_Thr/squared_norm_of_recon_L{model.numIter}_act_{not model.no_activation}_2.pdf\"\n",
    "    # plot_squared_norm_of_recon(x_layer_i_list, step, filename=filename)\n",
    "    science_plot_squared_norm_of_recon(x_layer_i_list, step, filename=filename)\n",
    "    # plot_squared_norm_of_recon(x_layer_i_list, step, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numIter_list = [15, 100, 500, 1000]  # Number of iterations\n",
    "steps = [1, 2, 2, 2]\n",
    "# steps = [1, 2, 10, 15]\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(\"../weights/model_parameters.pth\")\n",
    "\n",
    "for numIter, step in zip(numIter_list, steps):\n",
    "    print(\"*\" * 40)\n",
    "    print(f\"Num Iter: {numIter}\")\n",
    "    # No Activation function\n",
    "    model = LISTA(m, n, D, numIter, no_activation=True)\n",
    "    state_dict[\"thr\"] = torch.ones(numIter, 1)\n",
    "    model.load_state_dict(state_dict)\n",
    "    X_out, X_test, x_layer_i_list = evaluate_model(model, save=False)\n",
    "    # filename = f\"../Latex_Plots/squared_norm_of_recon_L{model.numIter}_act_{not model.no_activation}.pdf\"\n",
    "    # science_plot_squared_norm_of_recon(x_layer_i_list, filename=filename)\n",
    "\n",
    "    filename = f\"../figures/03_Impact_of_Soft_Thr/squared_norm_of_recon_L{model.numIter}_act_{not model.no_activation}_2.pdf\"\n",
    "    # filename = None\n",
    "    science_plot_squared_norm_of_recon(x_layer_i_list, step, filename=filename)\n",
    "    # plot_squared_norm_of_recon(x_layer_i_list, step, filename=filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser_cam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
